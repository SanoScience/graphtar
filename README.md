# GraphTar: applying word2vec and graph neural networks to miRNA target prediction
## Abstract
### Background
MicroRNAs (miRNAs) are short, non-coding RNA molecules that regulate gene expression by binding to specific mRNAs, inhibiting their translation. They play a critical role in regulating various biological processes and are implicated in many diseases, including cardiovascular, oncological, gastrointestinal diseases, and viral infections. Computational methods that can identify potential miRNA-mRNA interactions from raw data use one-dimensional miRNA-mRNA duplex representations and simple sequence encoding techniques, which may limit their performance.

### Results
We have developed GraphTar, a new target prediction method that uses a novel graph-based representation to reflect the spatial structure of the miRNA-mRNA duplex. Unlike existing approaches, we use the word2vec method to accurately encode RNA sequence information. In conjunction with a novel, word2vec-based encoding method, we use a graph neural network classifier that can accurately predict miRNA-mRNA interactions based on graph representation learning. As part of a comparative study, we evaluate three different node embedding approaches within the GraphTar framework and compare them with other state-of-the-art target prediction methods. The results show that the proposed method achieves similar performance to the best methods in the field and outperforms them on one of the datasets.

### Conclusions
In this study, a novel miRNA target prediction approach called GraphTar is introduced. Results show that GraphTar is as effective as existing methods and even outperforms them in some cases, opening new avenues for further research. However, the expansion of available datasets is critical for advancing the field towards real-world applications.
## Repository structure
The repository contains all the code used in the experiments. Specifically, it contains:
- dataset integration
- model configurations
- [NeptuneAI](https://neptune.ai/?utm_source=googleads&utm_medium=googleads&utm_campaign=[SG][HI][brand][rsa][all]&utm_term=neptune%20ai&gclid=Cj0KCQjwnf-kBhCnARIsAFlg49000HIobJdPqbh6uM67AFWssPBLd74m5mFSLZgRatRrBEVPc-fmQ6EaAgHCEALw_wcB) integration, for experiment tracking
- GraphTar method
- state-of-the-art algorithms reproduced for comparison with GraphTar:
  - [miRAW: A deep learning-based approach to predict microRNA targets by analyzing whole microRNA transcripts](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006185)
  - [DeepMirTar: a deep-learning approach for predicting human miRNA targets](https://academic.oup.com/bioinformatics/article/34/22/3781/5026656)
  - [miTAR: a hybrid deep learning-based approach for predicting miRNA targets](https://link.springer.com/article/10.1186/s12859-021-04026-6)
- analysis scripts
- experiment scripts

The directory structure is provided below, with short explanation for the most notable files and directories:
```
ðŸ“¦gnn-target-prediction
 â”£ ðŸ“‚analysis
 â”ƒ â”— ðŸ“‚results_analysis
 â”ƒ â”ƒ â”— ðŸ“œresults_analysis.ipynb <- notebook used to compute metric scores for the results section.
 â”£ ðŸ“‚data_modules
 â”ƒ â”£ ðŸ“‚configs <- directory with configuration per method-dataset combination. Configs specify data paths, transforms used, as well as train/test splits.
 â”ƒ â”ƒ â”£ ðŸ“œdataset_config.py
 â”ƒ â”ƒ â”£ ðŸ“œdeepmirtar_config.json
 â”ƒ â”ƒ â”£ ðŸ“œdeepmirtar_in_config.json
 â”ƒ â”ƒ â”£ ðŸ“œgraphtar_config_deepmirtar.json
 â”ƒ â”ƒ â”£ ðŸ“œgraphtar_config_deepmirtar_w2v.json
 â”ƒ â”ƒ â”£ ðŸ“œgraphtar_config_miraw.json
 â”ƒ â”ƒ â”£ ðŸ“œgraphtar_config_miraw_w2v.json
 â”ƒ â”ƒ â”£ ðŸ“œgraphtar_config_mirtarraw.json
 â”ƒ â”ƒ â”£ ðŸ“œgraphtar_config_mirtarraw_w2v.json
 â”ƒ â”ƒ â”£ ðŸ“œmiraw_config.json
 â”ƒ â”ƒ â”£ ðŸ“œmiraw_in_config.json
 â”ƒ â”ƒ â”£ ðŸ“œmirtarraw_config.json
 â”ƒ â”ƒ â”— ðŸ“œmitar_config.json
 â”ƒ â”£ ðŸ“‚datasets
 â”ƒ â”ƒ â”£ ðŸ“‚transforms <- all data transforms used for preprocessing
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚word2vec
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚models
 â”ƒ â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œgenerate_w2v_models.ipynb <- notebook used to generate word2vec models for every dataset-split combination
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œto_word2vec_embedding.py
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œjson_serializable.py
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œmerge.py
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œpad.py
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œto_one_hot.py
 â”ƒ â”ƒ â”ƒ â”— ðŸ“œto_tensor.py
 â”ƒ â”ƒ â”£ ðŸ“œinteraction_dataset.py <- Pytorch dataset implementation for interaction data used in reproduced methods
 â”ƒ â”ƒ â”— ðŸ“œinteraction_graph_dataset.py <- Pytorch dataset implementation for interaction data used in GraphTar (graph-based representation)
 â”ƒ â”£ ðŸ“œgraph_interaction_data_module.py <- Pytorch Lightning data module for GraphTar
 â”ƒ â”— ðŸ“œinteraction_data_module.py <- Pytorch Lightning data module for reproduced methods
 â”£ ðŸ“‚experiments <- all scripts used for training and evaluation in the reported experiments. For all reproduced methods, the structure follows the same pattern (see deepmirtar directory for details)
 â”ƒ â”£ ðŸ“‚deepmirtar
 â”ƒ â”ƒ â”£ ðŸ“œann.py <- ANN training script
 â”ƒ â”ƒ â”£ ðŸ“œann_test.py <- ANN evaluation script
 â”ƒ â”ƒ â”£ ðŸ“œrun_ann_training_tunning.sh <- SLURM script used for simple tests of ANN training logic
 â”ƒ â”ƒ â”£ ðŸ“œrun_array_ann_training_seed.sh <- SLURM script used to train final ANN part for every seed (after finding the best hyperparameters)
 â”ƒ â”ƒ â”£ ðŸ“œrun_array_ann_training_tunning.sh <- SLURM script used to train ANN part in the hyperparameters tunning phase
 â”ƒ â”ƒ â”£ ðŸ“œrun_array_autoencoder_training_seed.sh <- SLURM script used to train final SdA models for every seed (after finding the best hyperparameters)
 â”ƒ â”ƒ â”£ ðŸ“œrun_array_autoencoder_training_tunning.sh <- SLURM script used to train SdA part in the hyperparameters tunning phase
 â”ƒ â”ƒ â”£ ðŸ“œrun_autoencoder_training_tunning.sh <- SLURM script used for simple tests of SdA training logic
 â”ƒ â”ƒ â”— ðŸ“œsd_autoencoder.py <- SdA training script
 â”ƒ â”£ ðŸ“‚graphtar <- the structure follows the same pattern as for other methods, but files with 'w2v' in the file name use word2vec encoding, whereas files without 'w2v' use one-hot encoding
 â”ƒ â”ƒ â”£ ðŸ“œgnn.py
 â”ƒ â”ƒ â”£ ðŸ“œgnn_w2v.py
 â”ƒ â”ƒ â”£ ðŸ“œgnn_w2v_test.py
 â”ƒ â”ƒ â”£ ðŸ“œrun_array_graphtar_net_training_seed.sh
 â”ƒ â”ƒ â”£ ðŸ“œrun_array_graphtar_net_training_seed_w2v.sh
 â”ƒ â”ƒ â”£ ðŸ“œrun_array_graphtar_net_training_tunning.sh
 â”ƒ â”ƒ â”£ ðŸ“œrun_array_graphtar_net_training_tunning_w2v.sh
 â”ƒ â”ƒ â”£ ðŸ“œrun_graphtar_net_training_tunning.sh
 â”ƒ â”ƒ â”£ ðŸ“œrun_graphtar_net_training_tunning_local_w2v.sh
 â”ƒ â”ƒ â”— ðŸ“œrun_graphtar_net_training_tunning_w2v.sh
 â”ƒ â”£ ðŸ“‚miraw
 â”ƒ â”ƒ â”£ ðŸ“œann.py
 â”ƒ â”ƒ â”£ ðŸ“œann_test.py
 â”ƒ â”ƒ â”£ ðŸ“œautoencoder.py
 â”ƒ â”ƒ â”£ ðŸ“œrun_ann_training_tunning.sh
 â”ƒ â”ƒ â”£ ðŸ“œrun_array_ann_training_seed.sh
 â”ƒ â”ƒ â”£ ðŸ“œrun_array_ann_training_tunning.sh
 â”ƒ â”ƒ â”£ ðŸ“œrun_array_autoencoder_training_seed.sh
 â”ƒ â”ƒ â”£ ðŸ“œrun_array_autoencoder_training_tunning.sh
 â”ƒ â”ƒ â”— ðŸ“œrun_autoencoder_training_tunning.sh
 â”ƒ â”— ðŸ“‚mitar
 â”ƒ â”ƒ â”£ ðŸ“œmitar_net.py
 â”ƒ â”ƒ â”£ ðŸ“œmitar_net_test.py
 â”ƒ â”ƒ â”£ ðŸ“œrun_array_mitar_net_training_seed.sh
 â”ƒ â”ƒ â”£ ðŸ“œrun_array_mitar_net_training_tunning.sh
 â”ƒ â”ƒ â”— ðŸ“œrun_mitar_net_training_tunning.sh
 â”£ ðŸ“‚lightning_modules <- Pytorch Lightning modules for all methods
 â”ƒ â”£ ðŸ“‚deepmirtar
 â”ƒ â”ƒ â”£ ðŸ“œann.py
 â”ƒ â”ƒ â”— ðŸ“œsd_autoencoder.py
 â”ƒ â”£ ðŸ“‚graphtar
 â”ƒ â”ƒ â”— ðŸ“œgnn.py
 â”ƒ â”£ ðŸ“‚miraw
 â”ƒ â”ƒ â”£ ðŸ“œann.py
 â”ƒ â”ƒ â”— ðŸ“œautoencoder.py
 â”ƒ â”£ ðŸ“‚mitar
 â”ƒ â”ƒ â”— ðŸ“œmitar_net.py
 â”ƒ â”— ðŸ“‚models <- Pytorch models for all methods. Note, that for DeepMirTar and miRAW, models are divided into ann and autoencoder parts.
 â”ƒ â”ƒ â”£ ðŸ“‚deepmirtar
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œann.py
 â”ƒ â”ƒ â”ƒ â”— ðŸ“œdenoising_autoencoder.py
 â”ƒ â”ƒ â”£ ðŸ“‚graphtar
 â”ƒ â”ƒ â”ƒ â”— ðŸ“œgnn.py
 â”ƒ â”ƒ â”£ ðŸ“‚miraw
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œann.py
 â”ƒ â”ƒ â”ƒ â”— ðŸ“œautoencoder.py
 â”ƒ â”ƒ â”— ðŸ“‚mitar
 â”— â”— â”— â”— ðŸ“œmitar_net.py
```

## Data
## Models
## Experiments reproduction
### Environment setup
### Running models training
### Running models evaluation

